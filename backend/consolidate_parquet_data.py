"""
Script para consolidar dados dos arquivos Parquet gerados pelos ETLs
em um √∫nico arquivo Parquet compat√≠vel com o modelo de treinamento.
"""
import polars as pl
from pathlib import Path
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def consolidate_parquet_data(weather_parquet_path: Path, traffic_parquet_path: Path, output_path: Path):
    """
    L√™ os arquivos Parquet de clima e tr√¢nsito, os combina e salva um consolidado.
    
    Args:
        weather_parquet_path (Path): Caminho para weather_data.parquet.
        traffic_parquet_path (Path): Caminho para traffic_data.parquet.
        output_path (Path): Caminho para salvar o arquivo consolidado.
    """
    try:
        logger.info("Iniciando consolida√ß√£o de dados Parquet...")
        
        # 1. Ler os arquivos Parquet
        if not weather_parquet_path.exists():
            logger.error(f"‚ùå Arquivo de clima n√£o encontrado: {weather_parquet_path}")
            return
        if not traffic_parquet_path.exists():
            logger.error(f"‚ùå Arquivo de tr√¢nsito n√£o encontrado: {traffic_parquet_path}")
            return
            
        df_weather = pl.read_parquet(weather_parquet_path)
        df_traffic = pl.read_parquet(traffic_parquet_path)
        
        logger.info(f"‚úÖ Dados de clima carregados: {df_weather.height} linhas")
        logger.info(f"‚úÖ Dados de tr√¢nsito carregados: {df_traffic.height} linhas")
        
        # 2. Calcular m√©dias para ter um √∫nico valor por tipo de dado
        # Para simplificar, vamos pegar a √∫ltima entrada de cada um
        # Em um cen√°rio real, voc√™ pode querer fazer uma m√©dia ou outro tipo de agrega√ß√£o
        
        # Clima (pegando a √∫ltima linha)
        latest_weather = df_weather.tail(1)
        avg_temperature = latest_weather['temperature'][0]
        avg_humidity = latest_weather['humidity'][0]
        timestamp_str = latest_weather['timestamp'][0]
        
        # Converter timestamp para extrair dia da semana e m√™s
        dt_obj = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')) # Lidar com o formato ISO
        day_of_week = dt_obj.weekday()
        month = dt_obj.month
        
        # Tr√¢nsito (calculando a m√©dia do delay)
        avg_traffic_delay = df_traffic['delay'].mean()
        # Se avg_traffic_delay for null, usar 0
        if avg_traffic_delay is None:
            avg_traffic_delay = 0.0
        # Converter de segundos para minutos
        avg_traffic_delay_minutes = avg_traffic_delay / 60.0
        
        # 3. Criar um DataFrame com os dados consolidados no formato esperado pelo modelo
        # NOTA: Estamos criando dados sint√©ticos para 'iqv_overall' pois n√£o temos ele nos ETLs.
        # Em um cen√°rio real, voc√™ precisaria de uma forma de calcular ou obter esse valor.
        # Para fins de treinamento, vamos usar um valor fixo ou uma f√≥rmula simples.
        # Esta parte precisa ser ajustada com base na l√≥gica real do seu projeto.
        
        # Exemplo de c√°lculo simplificado de IQV (ajuste conforme necess√°rio)
        # Esta √© uma f√≥rmula arbitr√°ria para gerar um valor de target
        iqv_overall = (
            80  # Base
            - 0.5 * abs(avg_temperature - 22)  # Temperatura ideal √© 22¬∞C
            - 0.2 * abs(avg_humidity - 50)    # Humidade ideal √© 50%
            - 0.8 * avg_traffic_delay_minutes # Mais atraso = pior IQV
            + (10 if 6 <= month <= 8 else 0)  # Penalidade para inverno (exemplo)
        )
        iqv_overall = max(0, min(100, iqv_overall)) # Limitar entre 0 e 100
        
        # Criar o DataFrame consolidado
        consolidated_data = pl.DataFrame([{
            "temperature": avg_temperature,
            "humidity": avg_humidity,
            "traffic_delay": avg_traffic_delay_minutes, # Em minutos
            "day_of_week": day_of_week,
            "month": month,
            "iqv_overall": iqv_overall
        }])
        
        # Replicar os dados para ter v√°rias linhas (o modelo precisa de v√°rias amostras)
        # Vamos criar 100 linhas com pequenas varia√ß√µes
        import numpy as np
        np.random.seed(42)
        n_samples = 100
        variations = []
        for i in range(n_samples):
            temp_var = np.random.normal(0, 1)
            humidity_var = np.random.normal(0, 2)
            traffic_var = np.random.normal(0, 2) # minutos
            
            temp = max(-10, min(50, avg_temperature + temp_var))
            humidity = max(0, min(100, avg_humidity + humidity_var))
            traffic = max(0, avg_traffic_delay_minutes + traffic_var)
            
            # Recalcular IQV com base nas varia√ß√µes
            iqv = (
                80
                - 0.5 * abs(temp - 22)
                - 0.2 * abs(humidity - 50)
                - 0.8 * traffic
                + (10 if 6 <= month <= 8 else 0)
            )
            iqv = max(0, min(100, iqv))
            
            variations.append({
                "temperature": temp,
                "humidity": humidity,
                "traffic_delay": traffic,
                "day_of_week": day_of_week, # Mant√©m o mesmo para simplificar
                "month": month,             # Mant√©m o mesmo para simplificar
                "iqv_overall": iqv
            })
            
        consolidated_df = pl.DataFrame(variations)
        
        # 4. Salvar o DataFrame consolidado
        consolidated_df.write_parquet(output_path)
        logger.info(f"‚úÖ Dados consolidados salvos em: {output_path}")
        logger.info(f"üìä N√∫mero de amostras geradas: {consolidated_df.height}")
        logger.info("Visualiza√ß√£o das primeiras linhas:")
        print(consolidated_df.head())
        
    except Exception as e:
        logger.error(f"‚ùå Erro na consolida√ß√£o de dados Parquet: {e}")
        raise

def main():
    """Fun√ß√£o principal."""
    data_dir = Path(__file__).parent / "data"
    weather_file = data_dir / "weather_data.parquet"
    traffic_file = data_dir / "traffic_data.parquet"
    output_file = data_dir / "consolidated_data.parquet"
    
    consolidate_parquet_data(weather_file, traffic_file, output_file)

if __name__ == "__main__":
    main()